# ğŸŒ¿ Kannada BPE Tokenizer

A custom **Byte Pair Encoding (BPE)** tokenizer trained from scratch on a rich corpus of Kannada literature.
The tokenizer learns meaningful subword units directly from text and provides efficient segmentation, high compression, and expressive tokenization for Kannada NLP tasks.

---

## âœ¨ Features

* ğŸ”¤ **Fully custom BPE implementation** (no external tokenizer libraries)
* ğŸ“š Trained on **Kannada novels and long-form prose**
* ğŸ” Learns subword patterns directly from language statistics
* âš¡ **High compression ratio** â†’ efficient tokenization
* ğŸ›ï¸ Easy-to-use **encode** and **decode** API
* ğŸŒ Interactive demo on Hugging Face Spaces
* ğŸ“¦ Lightweight, CPU-friendly, pure-Python implementation

---

## ğŸ¥ Demo



![Demo](https://github.com/SXD390/Tokenizer_primitive_BPE_Kannada/blob/main/DATA/util/KN_BPE_Tokenizer_DEMO.gif)


---

## ğŸ“Š Training Summary

| Metric                    | Value                                      |
| ------------------------- | ------------------------------------------ |
| **Corpus size**           | ~140k characters (combined Kannada novels) |
| **Final vocabulary size** | **9002 tokens**                            |
| **Compression ratio**     | **3.7543** (chars / tokens)                |
| **Tokenizer type**        | Character-level BPE                        |
| **Training hardware**     | CPU-only                                   |

### ğŸ“˜ Compression Ratio Explained

Compression ratio tells how efficiently text is tokenized:

The compression ratio can be defined by the formula: $\text{compression ratio} = \frac{\text{total characters}}{\text{total tokens}}$.



A ratio of **3.75** means:
**Each token represents 3.75 original characters on average** â†’ excellent efficiency for Kannada.

---

## ğŸš€ Try the Tokenizer

### â–¶ï¸ Interactive Web Demo

Use the tokenizer directly in your browser:

ğŸ‘‰ **Hugging Face Space:**

```
https://huggingface.co/spaces/SXD390/BPE_KN_Tokenizer
```

### Example (Encoding)

**Input:**

```
à²¨à²®à²¸à³à²•à²¾à²°. à²¨à³€à²µà³ à²¹à³‡à²—à²¿à²¦à³à²¦à³€à²°à²¿?
```

**Output tokens:**
`[2123, 981, 7740, ...]`

**Compression ratio:**
`3.82`

### Example (Decoding)

```
[2123, 981, 7740, ...] â†’ "à²¨à²®à²¸à³à²•à²¾à²°. à²¨à³€à²µà³ à²¹à³‡à²—à²¿à²¦à³à²¦à³€à²°à²¿?"
```

*(Note: decoding is approximate because BPE merges tokens irreversibly.)*

---

## ğŸ§  How It Works

### 1ï¸âƒ£ Initial Character Vocabulary

The tokenizer begins with all unique Kannada Unicode characters plus an end-of-word marker.

### 2ï¸âƒ£ Pair Frequency Analysis

It scans the entire corpus to find the **most frequent adjacent character pairs**.

### 3ï¸âƒ£ Merge Operations

The most common pairs are merged into new tokens.
This process repeats until the target vocabulary size is reached.

### 4ï¸âƒ£ Tokenization

When encoding:

* Words are split into characters
* The learned merger rules are applied greedily
* Output tokens represent meaningful subword units

---

## ğŸ“ Project Structure

```
project/
â”œâ”€â”€ train_kannada_bpe.py      # BPE implementation + training script
â”œâ”€â”€ train_tokenizer.ipynb     # Notebook demonstration & reproducibility
â”œâ”€â”€ artifacts/
â”‚   â”œâ”€â”€ vocab.json            # Learned vocabulary
â”‚   â””â”€â”€ merges.json           # Learned BPE merge rules
â”œâ”€â”€ hf_space/
â”‚   â”œâ”€â”€ app.py                # Gradio app
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ vocab.json
â”‚   â””â”€â”€ merges.json
â””â”€â”€ txt_out/                  # Processed text files (if included)
```

---

## ğŸ›  Usage

### Install

```bash
pip install gradio
```

### Load the tokenizer

```python
from train_kannada_bpe import BPETokenizer

tokenizer = BPETokenizer()
tokenizer.load("vocab.json", "merges.json")
```

### Encode

```python
ids = tokenizer.encode("à²¨à²®à²¸à³à²•à²¾à²°")
print(ids)
```

### Decode

```python
text = tokenizer.decode(ids)
print(text)
```

---

## ğŸ“š Data Source

The tokenizer was trained on a manually assembled collection of publicly available Kannada novels in PDF â†’ text format.
Only the processed text is used; PDFs are not required.

---

---
## ğŸªª License

MIT License
