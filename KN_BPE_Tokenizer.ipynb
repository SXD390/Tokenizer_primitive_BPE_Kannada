{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71cf5d98-8e96-4fbb-a9ea-08e985484def",
   "metadata": {},
   "source": [
    "# Kannada BPE Tokenizer Training\n",
    "\n",
    "This notebook reproduces training of a custom BPE tokenizer for Assignment 11.\n",
    "It uses the `train_kannada_bpe.py` script and displays:\n",
    "- Final vocab size\n",
    "- Compression ratio\n",
    "- Sample encode/decode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d930889-d841-4470-97ee-6b6e0c13818b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main] Loading corpus from: txt_out/combined.txt\n",
      "[main] Total non-empty lines: 6277\n",
      "[main] Train lines: 5649\n",
      "[main] Test  lines: 628\n",
      "[train] Number of words in training data: 178219\n",
      "[train] Initial vocab size (chars + </w>): 115\n",
      "[train] Merges: 100, current vocab size (without special tokens): 215\n",
      "[train] Merges: 200, current vocab size (without special tokens): 315\n",
      "[train] Merges: 300, current vocab size (without special tokens): 415\n",
      "[train] Merges: 400, current vocab size (without special tokens): 515\n",
      "[train] Merges: 500, current vocab size (without special tokens): 615\n",
      "[train] Merges: 600, current vocab size (without special tokens): 715\n",
      "[train] Merges: 700, current vocab size (without special tokens): 815\n",
      "[train] Merges: 800, current vocab size (without special tokens): 915\n",
      "[train] Merges: 900, current vocab size (without special tokens): 1015\n",
      "[train] Merges: 1000, current vocab size (without special tokens): 1115\n",
      "[train] Merges: 1100, current vocab size (without special tokens): 1215\n",
      "[train] Merges: 1200, current vocab size (without special tokens): 1315\n",
      "[train] Merges: 1300, current vocab size (without special tokens): 1415\n",
      "[train] Merges: 1400, current vocab size (without special tokens): 1515\n",
      "[train] Merges: 1500, current vocab size (without special tokens): 1615\n",
      "[train] Merges: 1600, current vocab size (without special tokens): 1715\n",
      "[train] Merges: 1700, current vocab size (without special tokens): 1815\n",
      "[train] Merges: 1800, current vocab size (without special tokens): 1915\n",
      "[train] Merges: 1900, current vocab size (without special tokens): 2015\n",
      "[train] Merges: 2000, current vocab size (without special tokens): 2115\n",
      "[train] Merges: 2100, current vocab size (without special tokens): 2215\n",
      "[train] Merges: 2200, current vocab size (without special tokens): 2315\n",
      "[train] Merges: 2300, current vocab size (without special tokens): 2415\n",
      "[train] Merges: 2400, current vocab size (without special tokens): 2515\n",
      "[train] Merges: 2500, current vocab size (without special tokens): 2615\n",
      "[train] Merges: 2600, current vocab size (without special tokens): 2715\n",
      "[train] Merges: 2700, current vocab size (without special tokens): 2815\n",
      "[train] Merges: 2800, current vocab size (without special tokens): 2915\n",
      "[train] Merges: 2900, current vocab size (without special tokens): 3015\n",
      "[train] Merges: 3000, current vocab size (without special tokens): 3115\n",
      "[train] Merges: 3100, current vocab size (without special tokens): 3215\n",
      "[train] Merges: 3200, current vocab size (without special tokens): 3315\n",
      "[train] Merges: 3300, current vocab size (without special tokens): 3415\n",
      "[train] Merges: 3400, current vocab size (without special tokens): 3515\n",
      "[train] Merges: 3500, current vocab size (without special tokens): 3615\n",
      "[train] Merges: 3600, current vocab size (without special tokens): 3715\n",
      "[train] Merges: 3700, current vocab size (without special tokens): 3815\n",
      "[train] Merges: 3800, current vocab size (without special tokens): 3915\n",
      "[train] Merges: 3900, current vocab size (without special tokens): 4015\n",
      "[train] Merges: 4000, current vocab size (without special tokens): 4115\n",
      "[train] Merges: 4100, current vocab size (without special tokens): 4215\n",
      "[train] Merges: 4200, current vocab size (without special tokens): 4315\n",
      "[train] Merges: 4300, current vocab size (without special tokens): 4415\n",
      "[train] Merges: 4400, current vocab size (without special tokens): 4515\n",
      "[train] Merges: 4500, current vocab size (without special tokens): 4615\n",
      "[train] Merges: 4600, current vocab size (without special tokens): 4715\n",
      "[train] Merges: 4700, current vocab size (without special tokens): 4815\n",
      "[train] Merges: 4800, current vocab size (without special tokens): 4915\n",
      "[train] Merges: 4900, current vocab size (without special tokens): 5015\n",
      "[train] Merges: 5000, current vocab size (without special tokens): 5115\n",
      "[train] Merges: 5100, current vocab size (without special tokens): 5215\n",
      "[train] Merges: 5200, current vocab size (without special tokens): 5315\n",
      "[train] Merges: 5300, current vocab size (without special tokens): 5415\n",
      "[train] Merges: 5400, current vocab size (without special tokens): 5515\n",
      "[train] Merges: 5500, current vocab size (without special tokens): 5615\n",
      "[train] Merges: 5600, current vocab size (without special tokens): 5715\n",
      "[train] Merges: 5700, current vocab size (without special tokens): 5815\n",
      "[train] Merges: 5800, current vocab size (without special tokens): 5915\n",
      "[train] Merges: 5900, current vocab size (without special tokens): 6015\n",
      "[train] Merges: 6000, current vocab size (without special tokens): 6115\n",
      "[train] Merges: 6100, current vocab size (without special tokens): 6215\n",
      "[train] Merges: 6200, current vocab size (without special tokens): 6315\n",
      "[train] Merges: 6300, current vocab size (without special tokens): 6415\n",
      "[train] Merges: 6400, current vocab size (without special tokens): 6515\n",
      "[train] Merges: 6500, current vocab size (without special tokens): 6615\n",
      "[train] Merges: 6600, current vocab size (without special tokens): 6715\n",
      "[train] Merges: 6700, current vocab size (without special tokens): 6815\n",
      "[train] Merges: 6800, current vocab size (without special tokens): 6915\n",
      "[train] Merges: 6900, current vocab size (without special tokens): 7015\n",
      "[train] Merges: 7000, current vocab size (without special tokens): 7115\n",
      "[train] Merges: 7100, current vocab size (without special tokens): 7215\n",
      "[train] Merges: 7200, current vocab size (without special tokens): 7315\n",
      "[train] Merges: 7300, current vocab size (without special tokens): 7415\n",
      "[train] Merges: 7400, current vocab size (without special tokens): 7515\n",
      "[train] Merges: 7500, current vocab size (without special tokens): 7615\n",
      "[train] Merges: 7600, current vocab size (without special tokens): 7715\n",
      "[train] Merges: 7700, current vocab size (without special tokens): 7815\n",
      "[train] Merges: 7800, current vocab size (without special tokens): 7915\n",
      "[train] Merges: 7900, current vocab size (without special tokens): 8015\n",
      "[train] Merges: 8000, current vocab size (without special tokens): 8115\n",
      "[train] Merges: 8100, current vocab size (without special tokens): 8215\n",
      "[train] Merges: 8200, current vocab size (without special tokens): 8315\n",
      "[train] Merges: 8300, current vocab size (without special tokens): 8415\n",
      "[train] Merges: 8400, current vocab size (without special tokens): 8515\n",
      "[train] Merges: 8500, current vocab size (without special tokens): 8615\n",
      "[train] Merges: 8600, current vocab size (without special tokens): 8715\n",
      "[train] Merges: 8700, current vocab size (without special tokens): 8815\n",
      "[train] Merges: 8800, current vocab size (without special tokens): 8915\n",
      "[train] Training complete. Final vocab size (including special tokens): 9002\n",
      "[main] Final vocab size (including special tokens): 9002\n",
      "------------------------------------------------------\n",
      "[eval] Total characters in test set: 150304\n",
      "[eval] Total tokens in test set:     40035\n",
      "[eval] Compression ratio (chars/tokens): 3.7543\n",
      "------------------------------------------------------\n",
      "[eval] ✅ Compression ratio >= 3.2 requirement satisfied (for this test split).\n",
      "[main] Saved vocab to:  artifacts/vocab.json\n",
      "[main] Saved merges to: artifacts/merges.json\n"
     ]
    }
   ],
   "source": [
    "!python3 train_kannada_bpe.py \\\n",
    "    --data_path txt_out/combined.txt \\\n",
    "    --output_dir artifacts \\\n",
    "    --vocab_size 9000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34f58a09-737c-491a-b0ec-59f4640ea400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: ನಮಸ್ಕಾರ. ನೀವು ಹೇಗಿದ್ದೀರಿ?\n",
      "Token IDs: [3841, 1651, 140, 4162, 8092, 8371, 6161, 238]\n",
      "Decoded: ನಮಸ್ಕಾರ. ನೀವು ಹೇಗಿದ್ದೀರಿ?\n"
     ]
    }
   ],
   "source": [
    "from train_kannada_bpe import BPETokenizer\n",
    "\n",
    "tokenizer = BPETokenizer()\n",
    "tokenizer.load(\"artifacts/vocab.json\", \"artifacts/merges.json\")\n",
    "\n",
    "sample = \"ನಮಸ್ಕಾರ. ನೀವು ಹೇಗಿದ್ದೀರಿ?\"\n",
    "ids = tokenizer.encode(sample)\n",
    "decoded = tokenizer.decode(ids)\n",
    "\n",
    "print(\"Sample:\", sample)\n",
    "print(\"Token IDs:\", ids[:40])\n",
    "print(\"Decoded:\", decoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "529088df-acbe-435c-8c18-8d2463e485f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 9002\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocab size:\", len(tokenizer.vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305353da-8791-410b-99e8-b835a84ea036",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
